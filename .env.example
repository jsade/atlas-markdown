# Atlas Markdown Configuration
# https://github.com/jsade/atlas-markdown

# This scraper is designed specifically for Atlassian support documentation.
# The BASE_URL must:
#   1. Start with "https://support.atlassian.com/"
#   2. Include a specific product endpoint (not just the root URL)
#
# Valid examples:
#   - https://support.atlassian.com/jira-service-management-cloud
#   - https://support.atlassian.com/jira-software-cloud
#   - https://support.atlassian.com/confluence-cloud
#   - https://support.atlassian.com/jira-work-management
#   - https://support.atlassian.com/trello
#   - https://support.atlassian.com/bitbucket-cloud
#   - https://support.atlassian.com/statuspage
#
# Base URL for the documentation to scrape
BASE_URL=https://support.atlassian.com/jira-service-management-cloud

# Output directory for downloaded documentation. It will be created if it
# doesn't exist.
OUTPUT_DIR=./output

# Logging level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# Enable file logging (true/false)
# When enabled, logs will be written to files in LOG_DIR
LOG_ENABLED=false

# Directory for log files (created if it doesn't exist)
# Log files are named with timestamp: scraper_YYYY-MM-DD_HH-MM-SS.log
LOG_DIR=logs/

# =============================================================================
# You should not change the following settings unless you know what you're
#  doing. They are set to reasonable defaults for most use cases and to not
#  overload the Atlassian servers.
# =============================================================================

# Number of concurrent workers for downloading
WORKERS=5

# Delay between requests (in seconds)
REQUEST_DELAY=1.5

# User agent string
USER_AGENT="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"

# ============================================
# Safety Constraints - Prevent Runaway Scraping
# ============================================

# Maximum crawl depth from the entry point (0 = unlimited)
# This prevents the scraper from following links too deep into the site
MAX_CRAWL_DEPTH=5

# Maximum total pages to scrape (0 = unlimited)
# Hard limit to prevent accidental full-site scraping
MAX_PAGES=1000

# Maximum time to run in minutes (0 = unlimited)
# Stops the scraper after this many minutes regardless of progress
MAX_RUNTIME_MINUTES=120

# Maximum file size to download in MB
# Skip files larger than this to prevent disk space issues
MAX_FILE_SIZE_MB=50

# Domain restriction mode (product, any-atlassian, off)
# - product: Only URLs under your specific product (recommended)
#   Example: If BASE_URL is .../jira-service-management-cloud,
#            only allows .../jira-service-management-cloud/* URLs
# - any-atlassian: Allow any support.atlassian.com URL
#   Warning: May scrape other products' documentation!
# - off: No restriction (dangerous - not recommended)
DOMAIN_RESTRICTION=product

# Maximum retries for failed pages
# Prevents infinite retry loops
MAX_RETRIES=3

# Maximum consecutive failures before stopping
# Stops scraper if too many pages fail in a row (possible site issue)
MAX_CONSECUTIVE_FAILURES=20

# Enable dry run by default for safety (true/false)
# Set to false to actually download content
DRY_RUN_DEFAULT=false
